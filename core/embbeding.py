# XPOLION - Intelligent Query & Retrieval with Reasoning
# Features: Embedding + Search + Answer Generation

import os
import time
from dotenv import load_dotenv
from pinecone import Pinecone, ServerlessSpec
import google.generativeai as genai

from llm_handeler import query_gemini_flash

load_dotenv()

# Configure Gemini
genai.configure(api_key=os.getenv("GEMINI_KEY"))
EMBED_MODEL = "models/embedding-001"

# Configure Pinecone
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
INDEX_NAME = os.getenv("PINECONE_INDEX", "doc-embeddings")

# Initialize Pinecone
pc = Pinecone(api_key=PINECONE_API_KEY)

# Create index if missing
if INDEX_NAME not in [idx["name"] for idx in pc.list_indexes()]:
	print(f"[INFO] Creating Pinecone index: {INDEX_NAME}")
	pc.create_index(
		name=INDEX_NAME,
		dimension=768,
		metric="cosine",
		spec=ServerlessSpec(cloud="aws", region="us-east-1")
	)
	print("[INFO] Waiting for index to be ready...")
	time.sleep(10)

index = pc.Index(INDEX_NAME)
print(f"[INFO] Connected to Pinecone index: {INDEX_NAME}")


def get_gemini_embedding(text: str) -> list:
	response = genai.embed_content(model=EMBED_MODEL, content=text)
	embedding = response.get("embedding")
	if not embedding:
		raise ValueError("No embedding returned from Gemini.")
	return embedding

# Upsert document (store text in metadata)
# basically stores data in pinecone database
def upsert_document(doc_id: str, text: str, metadata: dict = None):
	embedding = get_gemini_embedding(text)
	full_metadata = metadata or {}
	full_metadata["text"] = text
	response = index.upsert(
		vectors=[{
			"id": doc_id,
			"values": embedding,
			"metadata": full_metadata
		}]
	)
	print(f"[INFO] Upserted {doc_id}, response: {response}")


def semantic_search(query: str, top_k: int = 3):
	embedding = get_gemini_embedding(query)
	results = index.query(
		vector=embedding,
		top_k=top_k,
		include_metadata=True
	)
	return [
		{
			"id": m["id"],
			"score": round(m["score"], 4),
			"source": m["metadata"].get("source", "unknown"),
			"text": m["metadata"].get("text", "No text")
		}
		for m in results["matches"]
	]


# ### FOR EASY ACCESS:
def embed(doc_id: str, query: str, context: str, meta: dict = None) -> dict:
	"""
	Inserts a document into Pinecone, performs semantic search, and retrieves a reasoning-based answer.

	Args:
		doc_id (str): Unique identifier for the document to upsert.
		query (str): The user query to search for.
		context (str): The content text to be embedded and stored.
		meta (dict, optional): Additional metadata for the document (e.g., {"source": "policy.pdf"}).

	Returns:
		dict: A dictionary containing:
			- "matches" (list): Retrieved context chunks with metadata and scores.
			- "answer" (str): The final answer generated by the LLM.
	"""
	upsert_document(doc_id, context, meta)
	matches = semantic_search(query)
	final_answer = query_gemini_flash(query, matches)
	return {"matches": matches, "answer": final_answer}



# # Example usage

# if __name__ == "__main__":
#     upsert_document(
#         "doc1",
#         "Flood damage is covered under natural disaster clause.",
#         {"source": "policy.pdf"}
#     )
#     query = "Does the policy cover flood damage?"
#     matches = semantic_search(query)
#     print("\n[INFO] Retrieved Chunks:", matches)
#     final_answer = query_gemini_flash(query, matches)
#     print("\n[FINAL ANSWER]:", final_answer)
